% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/AllClasses.R
\docType{class}
\name{ModelResults-class}
\alias{ModelResults-class}
\title{ModelResults class}
\description{
ModelResults class
}
\section{Slots}{

\describe{
\item{\code{model.input}}{An object of the modelInput class.}

\item{\code{pooling.filter}}{A matrix that pools convolution results.}

\item{\code{iteration.tracking}}{A data frame to track the iteration, weight, and error
values for each iteration of training.}

\item{\code{max.iterations}}{Maximum number of iterations.}

\item{\code{convergence.cutoff}}{Cutoff for convergence.}

\item{\code{learning.rate}}{Learning rate used during training.}

\item{\code{activation.type}}{Character value. Must be "sigmoid", "tanh", or "softmax".}

\item{\code{current.weights}}{Weights used in the current iteration.}

\item{\code{previous.weights}}{Weights used in the previous iteration.}

\item{\code{current.gradient}}{Gradient calculated for this iteration.}

\item{\code{weights.after.pooling}}{Whether to include the weights after the pooling
layer (as opposed to before). Must be a boolean.}

\item{\code{outcome.prediction}}{The prediction of the outcome.}

\item{\code{optimization.type}}{One of "BGD", "momentum", "adagrad", or "adam"}

\item{\code{current.iteration}}{Iteration (changes at each time step)}

\item{\code{previous.momentum}}{Momentum value used in momentum optimization}

\item{\code{previous.update.vector}}{Previous value used to update weights, used in ADAM
optimization}

\item{\code{sum.square.gradients}}{Sum of squared gradients over iterations, used in
Adagrad optimization}
}}

